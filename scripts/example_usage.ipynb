{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9afc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RayRoPE Demo: Self-Attention for Multi-View Images\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pos_enc.rayrope import RayRoPE_DotProductAttention\n",
    "from pos_enc.utils.rayrope_mha import MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RayRoPETransformer(nn.Module):\n",
    "    \"\"\"A minimal transformer using RayRoPE multi-view self-attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, num_layers, patches_x, patches_y, img_w, img_h):\n",
    "        super().__init__()\n",
    "        head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Single RayRoPE module shared across all layers\n",
    "        self.rayrope_attn = RayRoPE_DotProductAttention(\n",
    "            head_dim=head_dim, patches_x=patches_x, patches_y=patches_y,\n",
    "            image_width=img_w, image_height=img_h,\n",
    "            pos_enc_type='d_pj+0_3d', num_rays_per_patch=3, depth_type='predict_dsig',\n",
    "        )\n",
    "        \n",
    "        # MHA layers all use the same RayRoPE attention function\n",
    "        self.mha_layers = nn.ModuleList([\n",
    "            MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads,\n",
    "                               predict_d='predict_dsig', sdpa_fn=self.rayrope_attn.forward)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(embed_dim) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x, w2cs, Ks):\n",
    "        # Precompute RayRoPE encodings once from camera poses\n",
    "        self.rayrope_attn._precompute_and_cache_apply_fns(w2cs, Ks)\n",
    "        \n",
    "        for mha, norm in zip(self.mha_layers, self.norms):\n",
    "            x = x + mha(norm(x), norm(x), norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model (head_dim=48 must be multiple of 24 for RayRoPE)\n",
    "model = RayRoPETransformer(\n",
    "    embed_dim=768, num_heads=8, num_layers=2,\n",
    "    patches_x=8, patches_y=8, img_w=128, img_h=128,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data: 2 batches, 3 cameras, 8x8 patches, 128x128 images\n",
    "B, num_cams, patches, img_size, embed_dim = 2, 3, 8, 128, 768\n",
    "\n",
    "x = torch.randn(B, num_cams * patches * patches, embed_dim).cuda()  # Patch embeddings\n",
    "\n",
    "w2cs = torch.eye(4).expand(B, num_cams, -1, -1).clone().cuda()      # Camera extrinsics\n",
    "w2cs[:, :, :3, 3] = torch.randn(B, num_cams, 3) * 0.5               # Random translation\n",
    "\n",
    "Ks = torch.zeros(B, num_cams, 3, 3).cuda()                          # Camera intrinsics\n",
    "Ks[:, :, 0, 0] = Ks[:, :, 1, 1] = 500                               # fx, fy\n",
    "Ks[:, :, 0, 2] = Ks[:, :, 1, 2] = img_size / 2                      # cx, cy\n",
    "Ks[:, :, 2, 2] = 1\n",
    "\n",
    "print(f\"Input: x {x.shape}, w2cs {w2cs.shape}, Ks {Ks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadcdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    out = model(x, w2cs, Ks)\n",
    "\n",
    "print(f\"Output: {out.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rayrope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
